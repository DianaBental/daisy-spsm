:- module(callDemoProcesses, [respond_to_query/0]).

% :- use_module(library(system3)). % SICSTUS
:- use_module(library(lists)).

:- use_module('../queryRespond/utils.pl').

:- use_module('../queryRespond/translation.pl').
:- use_module('../queryRespond/handleDatasetNames.pl').
:- use_module('../queryRespond/wordnetSearch.pl').
:- use_module('../queryRespond/sumoSearch.pl').
:- use_module('../queryRespond/spsmMatch.pl').
:- use_module('../queryRespond/narrowDatasets.pl').
:- use_module('../queryRespond/repair.pl').

% Dangerous but this stops SICSTUS from complaining and pausing the run
% whenever dynamically
% created data is reconsulted. This flag doesn't seem to affect SWI Prolog
% which just prints a warning and carries on.
:- set_prolog_flag(redefine_warnings, off).


% The file type is set in translation.sh, either n3 or csv are known types
respond_to_query :-
    environment_variable('DAISY_FILETYPE', TargetFilesType),
    respond_to_query(TargetFilesType).

respond_to_query(TargetFilesType) :-
    new_output,
    query_path('query.pl', QueryFile),
    open(QueryFile, read, QF),
    read(QF, QueryTerm),
    close(QF),
    nl,
    write('Incoming query: '), writeq(QueryTerm), nl,
    nl,
    append_to_output(['Query: ', QueryTerm]),
    write_heading(['Try the initial query on the target dataset']),
    wait_for_user,    

    nl, write('Make query schema'), nl,
    make_query_schema(QueryTerm, QuerySchema),
    write('Query schema: '), writeq(QuerySchema), nl, nl,

    % store the predicates/prefixes that are in the query dataset
    QueryTerm =.. [QueryDatasetName|_],
    %% make_source_prefixes(TargetFilesType, QueryDatasetName),
    %% load_source_prefixes,

    start_translation(TargetFilesType, QueryTerm, Success),
    %%    handle_query_result(Success, TargetFilesType, QueryTerm, SplitQueryNames, QuerySchema).
    wait_for_user,    
    handle_query_result(Success, TargetFilesType, QueryTerm, QuerySchema).

handle_query_result(success, _TargetFilesType, _QueryTerm, _QuerySchema) :- !. % Do nothing, we've won.
handle_query_result(_NotSuccess, TargetFilesType, QueryTerm, QuerySchema) :-
    write_heading(['Expand the query schema']),

    % store the query dataset filename and a version split into words
    make_query_names_file(QueryTerm, [dataset(QueryName, SplitQueryNames)]),
    
    nl, write('Expand the query dataset term using Wordnet search'), nl,
    connect_list_to_words(SplitQueryNames, WordnetAssociatedTerms),
    nl, write('WordNet expanded the query dataset \''), write(QueryName), write('\' to '), nl, write(WordnetAssociatedTerms), nl,
%    nl, write('Trace Wordnet Test'), nl, 
%    connect_list_to_words([water, body, measures], WNTest),
    %    write(WNTest), nl,
    wait_for_user,        
    nl, write('Expand the query dataset term using Sumo search'), nl,
    sumo_connection(SplitQueryNames, SumoRelatedWordSets),
    nl, write('Sumo expanded the query dataset \''), write(QueryName), write('\' to '), nl, write(SumoRelatedWordSets), nl,

    wait_for_user,        
    write_heading(['Identify target datasets related to the query']),
    % Now get all the dataset names.
%    nl, write('Find target dataset names'), nl,
    make_target_names_file(TargetFilesType, _TargetNames, SplitTargetNames),
    wait_for_user,        
    nl, write('Narrowing the datasets to choose ones related to query'), nl,
    narrow_datasets(SplitTargetNames, WordnetAssociatedTerms, SumoRelatedWordSets, Datasets),
    nl, write('Narrowed to these target datasets: '), write(Datasets), nl,
    append_to_output_nl(['Narrowed to these target datasets: '|Datasets]),
    write_heading(['Make schemas for the target datasets']),
    make_target_candidates(TargetFilesType, Datasets, Candidates),
    nl, write('Schemas for the selected (narrowed) datasets are: '), nl,
    wait_for_user,
    write_separated_list(Candidates), nl,
    write_heading(['Use SPSM to match the query and selected target schemas']),
    rank_matches(QuerySchema, Candidates, Maps), %SPSM
    wait_for_user,
    repair_schema(QueryTerm, QuerySchema, Maps, NewSchemaQueries),
    write_heading(['Try the mapped queries.']),
    try_mapped_queries(NewSchemaQueries, TargetFilesType).

% try_mapped_queries(+Queries, +TargetFilesType)
%
% Try a set of queries (which have been generated by the schema repair
% process), one at a time.
% We try *all* the possible schema repairs. 
% If a query initially fails due to no matching data, make data repairs
% and try again. 
% For each schema repair we may do several data repairs, but we stop
% doing data repairs as soon as we find one that returns some data; and we
% move on to the next schema repair.
try_mapped_queries([], _) :- !.
try_mapped_queries([Query|Queries], TargetFilesType) :-
    % Do the query
    wait_for_user,        
    try_query(TargetFilesType, Query, Success),
    % Do we need to do data repairs? If so, do them until something matches.
    consider_data_repair(TargetFilesType, Query, Success),
    % Do another query
    try_mapped_queries(Queries, TargetFilesType).

% Run a single query.
try_query(TargetFilesType, Query, Success) :-
    Query =.. [Dataset|_Args],
    load_prefixes(Dataset),
    start_translation(TargetFilesType, Query, Success),
    !. % Only try a query once. Do not fail back into this. 

% Do data repairs until one of them succeeds and returns some data
% (or there are no more possible data repairs.)
%
% It's only worth doing data repair if the query operated successfully but
% it returned no data. If it succeeded (i.e. it returned data) or failed
% altogether then data repair is not appropriate.
consider_data_repair(TargetFilesType, Query, no_data) :- 
    % Generate a possible data repair.
    % If try_query fails, repair_data may retry and generate other repairs.
    repair_data(Query, NewQuery),
    try_query(TargetFilesType, NewQuery, success).
consider_data_repair(_, Query, no_data) :- 
    !,
    % We know all the columns exist, and we already tried a data repair
    % with all the columns uninstantiated. So this situation shouldn't
    % arise in most datasets.
    % But it could arise in an n3 file if no single record has data
    % in all the columns that appear in the query.
    % I suppose repair_data could deal with it by dropping
    % columns from the query until some combo matches - but I haven't.
    write_heading(['No data for query ', Query]).
consider_data_repair(_Type, _Query, success) :- !.
consider_data_repair(_Type, _Query, failure) :- !.

% repair_schema(+QueryTerm, +QuerySchema, +Mappings, -NewQueries)
%
% The schema repair process.
% Generate replacement queries using the schema Mappings.
% QuerySchema doesn't actually get used but it's handy for reporting.
% The replacement is done directly on the QueryTerm to create a
% set of new query terms in which the dataset and column names have been
% replaced. If there are any values for any of the columns, these are
% maintained unchanged in the new queries.
repair_schema(QueryTerm, QuerySchema, [], _) :-
    !,
    nl, write('No matches found for '),
    write(QuerySchema), nl,
    write('Cannot respond to '),
    write(QueryTerm), nl,
    halt.
repair_schema(QueryTerm, QuerySchema, Mappings, NewQueries) :-
    nl, write('For query schema '), write(QuerySchema),
    nl, write('Found schema mappings: '), nl, nl,
    write_separated_list(Mappings), nl,
    wait_for_user,
    write_heading(['Repair the query according to the schema mappings']),
    repair_with_schema_maps(QueryTerm, Mappings, NewQueries),
    nl, nl, write('The query can be repaired in the following ways:'), nl,
    write_list(NewQueries).

% make_target_candidates(+TargetFilesType, +Datasets, -Schemas)
%
% Creates a schema (i.e. a list of column names) for each dataset in the
% input list.
%
% As a side effect, it creates a set of files (one per dataset) which
% contain rdf-prefix mappings for each column. These will come in useful
% when creating the "repaired" queries.
make_target_candidates(_, [], []) :- !.
make_target_candidates(TargetFilesType, [Dataset|T], [Schema|Candidates]) :-
    wait_for_user,
    make_target_schema(TargetFilesType, Dataset, Schema),
    make_target_candidates(TargetFilesType, T, Candidates).


% Take the query and create a schema from it.
% i.e. make a query with no data values, and no subject info,
% just the tablename/rdf-filename and column-headings/rdf-predicate names.
% make_query_schema(+QueryTerm, -QuerySchema) :-
make_query_schema(QueryTerm, QuerySchema) :-
    ground(QueryTerm),
    !,
    QueryTerm =.. [Functor | QueryArgs],
    make_query_schema_args(QueryArgs, QSchemaArgs),
    QuerySchema =.. [Functor|QSchemaArgs].
make_query_schema(QueryTerm, _QuerySchema) :-
    \+ ground(QueryTerm),
    nl, write('The query '),
    writeq(QueryTerm),
    write(' contains a Prolog variable. All terms in the query that start with a capital letter must be enclosed in single quote marks.'),
    nl,
    halt.

make_query_schema_args([], []) :- !.
% Leave out explicit references to a "subject" (only expected to apply to
% Sparql type queries)
make_query_schema_args([Arg|QueryArgs], QueryFunctors) :-
    functor(Arg, subject, _),
    !,
    make_query_schema_args(QueryArgs, QueryFunctors).
make_query_schema_args([Arg|QueryArgs], [Functor|QueryFunctors]) :-
    functor(Arg, Functor, _),
    make_query_schema_args(QueryArgs, QueryFunctors).

